{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "**IGNORE**\n",
    "\n",
    "First we need to import some stuff. Feel free to ignore this cell.\n",
    "\n",
    "If you're interested, each import has an associated comment that explains why\n",
    "the import is useful/necessary.\n",
    "\"\"\"\n",
    "\n",
    "# Needed for various filesystem tasks (os.path.exists etc.)\n",
    "import os\n",
    "# Used for checking how many cores are available for processing.\n",
    "import multiprocessing\n",
    "# Used for constructing paths.\n",
    "from pathlib import Path\n",
    "\n",
    "# Essential for all mathematical operations we'll be carrying out.\n",
    "import numpy as np\n",
    "\n",
    "# diffraction_utils is a library developed at Diamond by Richard Brearton\n",
    "# (richard.brearton@diamond.ac.uk) to ease the task of parsing data files and\n",
    "# carrying out some common calculations. Here, we'll be using it to define\n",
    "# frames of reference, and parse nexus files.\n",
    "# We also use diffraction_utils' Region object to specify regions of interest/\n",
    "# background regions.\n",
    "from diffraction_utils import Frame, Region\n",
    "\n",
    "# The following imports are required for the core of the calculation code, also\n",
    "# written by Richard Brearton (richard.brearton@diamond.ac.uk).\n",
    "# This is the central Experiment object, which stores all the logic related to\n",
    "# mapping the experiment.\n",
    "from fast_rsm.experiment import Experiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "**ESSENTIAL**\n",
    "\n",
    "This cell requires action! Make sure you set all of the variables defined here.\n",
    "\"\"\"\n",
    "\n",
    "# What was your scattering geometry/how was your sample mounted? Options are\n",
    "# 'horizontal', 'vertical' and 'DCD'.\n",
    "setup = 'vertical'\n",
    "\n",
    "# Set local_data_path if your data isn't stored on the diamond system any more\n",
    "# (for example if it's on a memory stick or scratch drive).\n",
    "local_data_path = None\n",
    "# Set this if you want to save the output somewhere other than the processing\n",
    "# folder. Be warned, this could take up a lot of space.\n",
    "local_output_path = None\n",
    "\n",
    "# If you're processing on the cluster, you need to populate the next few fields.\n",
    "# The experiment number, used to work out where your data is stored.\n",
    "experiment_number = 'si31429-1'\n",
    "\n",
    "# The sub-directory containing your experimental data. Leave as None if unused.\n",
    "# Otherwise, if the data was stored in a subdirectory called \"day_1\", e.g.\n",
    "#   /dls/i07/data/2022/si32333-1/day_1/\n",
    "# then you should use:\n",
    "#   data_sub_directory = \"day_1\"\n",
    "data_sub_directory = \"sample1/\"\n",
    "\n",
    "# The year the experiment took place.\n",
    "year = 2022\n",
    "\n",
    "# The scan numbers of the scans that we want to use to produce this reciprocal\n",
    "# space map. For example, the default value of scan_numbers shows how to specify\n",
    "# every scan between number 421772 and 421778 inclusive, but skipping scan\n",
    "# number 421776.\n",
    "scan_numbers = [446509]\n",
    "\n",
    "# Uncomment the following to set scan_numbers equal to every scan number between\n",
    "# scan_start and scan_stop (inclusive of scan_stop):\n",
    "# scan_start = 439168\n",
    "# scan_stop = 439176\n",
    "# scan_numbers = list(range(scan_start, scan_stop + 1))\n",
    "\n",
    "# The beam centre, as can be read out from GDA, in pixel_x, pixel_y. If your\n",
    "# map looks wacky, you probably cocked this up.\n",
    "beam_centre = (243, 92)\n",
    "\n",
    "# The distance between the sample and the detector (or, if using the DCD, the\n",
    "# distance between the receiving slit and the detector). Units of meters.\n",
    "detector_distance = 930e-3\n",
    "\n",
    "# The frame/coordinate system you want the map to be carried out in.\n",
    "# Options for frame_name argument are:\n",
    "#     Frame.hkl     (map into hkl space - requires UB matrix in nexus file)\n",
    "#     Frame.sample_holder   (standard map into 1/Ã…)\n",
    "#     Frame.lab     (map into frame attached to lab.)\n",
    "#     Frame.qpar_qperp (as in Frame.lab, but with no component along beam)\n",
    "#\n",
    "# Please note that the q_parallel q_perpendicular frame is a bad choice. Q is a\n",
    "# three dimensional vector. By choosing this frame, you are consenting to\n",
    "# throwing one of these dimensions in the bin. Instead consider Frame.lab.\n",
    "#\n",
    "# Options for coordinates argument are:\n",
    "#     Frame.cartesian   (normal cartesian coords: hkl, Qx Qy Qz, etc.)\n",
    "#     Frame.polar       (cylindrical polar with cylinder axis along l/Qz)\n",
    "#\n",
    "# Frame.polar will give an output like a more general version of PyFAI.\n",
    "# Frame.cartesian is for hkl maps and Qx/Qy/Qz. Any combination of frame_name\n",
    "# and coordinates will work, so try them out; get a feel for them.\n",
    "frame_name = Frame.hkl\n",
    "coordinates = Frame.cartesian\n",
    "\n",
    "# How large would you like your output file to be, in MB? 100MB normally gives\n",
    "# very good resolution without sacrificing performance. If you want something\n",
    "# higher resolution, feel free, but be aware that the performance of the map and\n",
    "# the analysis will start to suffer above around 1GB.\n",
    "# Max file size is 2GB (2000MB).\n",
    "output_file_size = 50\n",
    "\n",
    "# This is for loading into binoculars. If set to false, .npy and .vtr files\n",
    "# will be saved, for manual analysis and paraview, respectively.\n",
    "save_binoculars_h5 = True\n",
    "\n",
    "# Are you using the DPS system?\n",
    "using_dps = False\n",
    "\n",
    "# The DPS central pixel locations are not typically recorded in the nexus file.\n",
    "# UNITS OF METERS, PLEASE (everything is S.I., except energy in eV).\n",
    "dpsx_central_pixel = 0\n",
    "dpsy_central_pixel = 0\n",
    "dpsz_central_pixel = 0\n",
    "\n",
    "# Note: THESE CAN BE HAPPILY AUTO CALCULATED.\n",
    "# These take the form:\n",
    "# volume_start = [h_start, k_start, l_start]\n",
    "# volume_stop = [h_stop, k_stop, l_stop]\n",
    "# volume_step = [h_step, k_step, l_step]\n",
    "# Leave as None if you don't want to specify them. You can specify whichever\n",
    "# you like (e.g. you can specify step and allow start/stop to be auto\n",
    "# calculated)\n",
    "volume_start = None\n",
    "volume_stop = None\n",
    "volume_step = None\n",
    "\n",
    "# Only use this if you need to load your data from a .dat file.\n",
    "load_from_dat = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "**MASKING**\n",
    "\n",
    "This cell contains details on how to mask pixels. You can either mask a series\n",
    "of individual pixels, mask rectangular regions of pixels, or dynamically mask\n",
    "pixels based on their intensity (not recommended).\n",
    "\"\"\"\n",
    "\n",
    "# If you have a small number of hot pixels to mask, specify them one at a time\n",
    "# in a list. In other words, it should look like:\n",
    "# specific_pixels = [(pixel_x1, pixel_y1), (pixel_x2, pixel_y2)]\n",
    "# Or, an exact example, where we want to mask pixel (233, 83) and pixel \n",
    "# (234, 83), where pixel coordinates are (x, y):\n",
    "# \n",
    "# specific_pixels = [\n",
    "#     (233, 83),\n",
    "#     (234, 83)\n",
    "# ]\n",
    "# \n",
    "# Leave specific pixels as None if you dont want to mask any specific pixels.\n",
    "# For this dataset we need to mask pixel (x=233, y=83)\n",
    "specific_pixels = None\n",
    "\n",
    "# If you want to specify an entire region of pixels to mask, do so here.\n",
    "# This is done using a \"Region\" object. To make a Region, give it start_x, \n",
    "# stop_x, start_y, start_y, as follows:\n",
    "# \n",
    "mask_1 = Region(0, 180, 0, 30)\n",
    "mask_2 = Region(320, -1, 165, -1)\n",
    "mask_3 = Region(0, -1, 0, 30)\n",
    "mask_4 = Region(0, -1, 165, -1)\n",
    " \n",
    "# Where my_mask_region runs in x from pixel 3 to 6 inclusive, and runs in y from\n",
    "# pixel 84 to 120 inclusive. You can make as many mask regions as you like, just\n",
    "# make sure that you put them in the mask_regions list, as follows:\n",
    "# mask_regions = [my_mask_region, Region(1, 2, 3, 4)]\n",
    "# \n",
    "# If you don't want to use any mask regions, just leave mask_regions equal to\n",
    "# None.\n",
    "mask_regions = None\n",
    "\n",
    "# Ignore pixels with an intensity below this value. If you don't want to ignore\n",
    "# any pixels, then set min_intensity = None. This is useful for dynamically\n",
    "# creating masks (which is really useful for generating masks from -ve numbers).\n",
    "min_intensity = 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "**IGNORE**\n",
    "This cell prepares the calculation. You probably shouldn't change anything here\n",
    "unless you know what you're doing.\n",
    "\"\"\"\n",
    "\n",
    "# Which synchrotron axis should become the out-of-plane (001) direction.\n",
    "# Defaults to 'y'; can be 'x', 'y' or 'z'.\n",
    "if setup == 'vertical':\n",
    "    oop = 'x'\n",
    "elif setup == 'horizontal':\n",
    "    oop = 'y'\n",
    "elif setup == 'DCD':\n",
    "    oop = 'y'\n",
    "else:\n",
    "    raise ValueError(\n",
    "        \"Setup not recognised. Must be 'vertical', 'horizontal' or 'DCD.\")\n",
    "\n",
    "if output_file_size > 2000:\n",
    "    raise ValueError(\"output_file_size must not exceed 2000. \"\n",
    "                     f\"Value received was {output_file_size}.\")\n",
    "\n",
    "# Max number of cores available for processing.\n",
    "num_threads = multiprocessing.cpu_count()\n",
    "\n",
    "# Work out where the data is.\n",
    "if local_data_path is None:\n",
    "    data_dir = Path(f\"/dls/i07/data/{year}/{experiment_number}/\")\n",
    "else:\n",
    "    data_dir = Path(local_data_path)\n",
    "# data_dir = Path(f\"/Users/richard/Data/i07/{experiment_number}/\")\n",
    "\n",
    "# Store this for later.\n",
    "if local_output_path is None:\n",
    "    processing_dir = data_dir / \"processing\"\n",
    "else:\n",
    "    processing_dir = Path(local_output_path)\n",
    "if data_sub_directory is not None:\n",
    "    data_dir /= Path(data_sub_directory)\n",
    "\n",
    "# Here we calculate a sensible file name that hasn't been taken.\n",
    "i = 0\n",
    "save_file_name = f\"mapped_scan_{scan_numbers[0]}_{i}\"\n",
    "save_path = processing_dir / save_file_name\n",
    "# Make sure that this name hasn't been used in the past.\n",
    "while (os.path.exists(str(save_path) + \".npy\") or\n",
    "       os.path.exists(str(save_path) + \".vtk\") or\n",
    "       os.path.exists(str(save_path) + \"_l.txt\") or\n",
    "       os.path.exists(str(save_path) + \"_tth.txt\") or\n",
    "       os.path.exists(str(save_path) + \"_Q.txt\") or\n",
    "       os.path.exists(save_path)):\n",
    "    i += 1\n",
    "    save_file_name = f\"mapped_scan_{scan_numbers[0]}_{i}\"\n",
    "    save_path = processing_dir / save_file_name\n",
    "\n",
    "    if i > 1e7:\n",
    "        raise ValueError(\n",
    "            \"Either you tried to save this file 10000000 times, or something \"\n",
    "            \"went wrong. I'm going with the latter, but exiting out anyway.\")\n",
    "\n",
    "\n",
    "# Work out the paths to each of the nexus files. Store as pathlib.Path objects.\n",
    "nxs_paths = [data_dir / f\"i07-{x}.nxs\" for x in scan_numbers]\n",
    "\n",
    "# Construct the Frame object from the user's preferred frame/coords.\n",
    "map_frame = Frame(frame_name=frame_name, coordinates=coordinates)\n",
    "\n",
    "# Prepare the pixel mask. First, deal with any specific pixels that we have.\n",
    "# Note that these are defined (x, y) and we need (y, x) which are the\n",
    "# (slow, fast) axes. So: first we need to deal with that!\n",
    "if specific_pixels is not None:\n",
    "    specific_pixels = specific_pixels[1], specific_pixels[0]\n",
    "\n",
    "# Now deal with any regions that may have been defined.\n",
    "# First make sure we have a list of regions.\n",
    "if isinstance(mask_regions, Region):\n",
    "    mask_regions = [mask_regions]\n",
    "\n",
    "# Now swap (x, y) for each of the regions.\n",
    "if mask_regions is not None:\n",
    "    for region in mask_regions:\n",
    "        region.x_start, region.y_start = region.y_start, region.x_start\n",
    "        region.x_end, region.y_end = region.y_end, region.x_end\n",
    "\n",
    "# Finally, instantiate the Experiment object.\n",
    "experiment = Experiment.from_i07_nxs(\n",
    "    nxs_paths, beam_centre, detector_distance, setup, \n",
    "    using_dps=using_dps)\n",
    "\n",
    "experiment.mask_pixels(specific_pixels)\n",
    "experiment.mask_regions(mask_regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "**POTENTIALLY REQUIRED**\n",
    "\n",
    "This cell is for changing metadata that is stored in, or inferred from, the\n",
    "nexus file. This is generally for more nonstandard stuff.\n",
    "\"\"\"\n",
    "\n",
    "for i, scan in enumerate(experiment.scans):\n",
    "    # Deal with the dps offsets.\n",
    "    if scan.metadata.data_file.using_dps:\n",
    "        scan.metadata.data_file.dpsx -= dpsx_central_pixel\n",
    "        scan.metadata.data_file.dpsy -= dpsy_central_pixel\n",
    "        scan.metadata.data_file.dpsz -= dpsz_central_pixel\n",
    "\n",
    "    # Load from .dat files if we've been asked.\n",
    "    if load_from_dat:\n",
    "        dat_path = data_dir / f\"{scan_numbers[i]}.dat\"\n",
    "        scan.metadata.data_file.populate_data_from_dat(dat_path)\n",
    "\n",
    "    # This is where you might want to overwrite some data that was recorded\n",
    "    # badly in the nexus file. See (commented out) examples below.\n",
    "    # scan.metadata.data_file.probe_energy = 12500\n",
    "    # scan.metadata.data_file.transmission = 0.4\n",
    "    # scan.metadata.data_file.using_dps = True\n",
    "    # scan.metadata.data_file.ub_matrix = np.array([\n",
    "    #     [1, 0, 0],\n",
    "    #     [0, 1, 0],\n",
    "    #     [0, 0, 1]\n",
    "    # ])\n",
    "\n",
    "    # Would you like to skip any images in any scans? Do so here!\n",
    "    # This shows how to skip the 9th in the 3rd scan (note the zero counting).\n",
    "    # if i == 2:\n",
    "    #     scan.skip_images.append(8)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "**IGNORE**\n",
    "\n",
    "This cell contains all of the logic for running the calculation. You shouldn't\n",
    "run this on your local computer, it'll either raise an exception or take\n",
    "forever.\n",
    "\"\"\"\n",
    "from fast_rsm.diamond_utils import save_binoculars_hdf5\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Calculate and save a binned reciprocal space map, if requested.\n",
    "    experiment.binned_reciprocal_space_map(\n",
    "        num_threads, map_frame, output_file_size=output_file_size, oop=oop,\n",
    "        min_intensity_mask=min_intensity,\n",
    "        output_file_name=save_path, \n",
    "        volume_start=volume_start, volume_stop=volume_stop,\n",
    "        volume_step=volume_step)\n",
    "\n",
    "    if save_binoculars_h5:\n",
    "        save_binoculars_hdf5(str(save_path) + \".npy\", str(save_path) + '.hdf5')\n",
    "        print(f\"\\nSaved BINoculars file to {save_path}.hdf5.\\n\")\n",
    "\n",
    "    # Finally, print that it's finished We'll use this to work out when the\n",
    "    # processing is done.\n",
    "    print(\"PROCESSING FINISHED.\")\n",
    "\n",
    "class DontContinue(Exception):\n",
    "    \"\"\"Raise to stop processing on the cluster at this cell\"\"\"\n",
    "\n",
    "raise DontContinue(\"Processing complete!!\\n\"\n",
    "                   \"This is intentionally raised to stop the processing. \"\n",
    "                   \"Never worry about the presence of this 'error'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "**ESSENTIAL**\n",
    "\n",
    "This is the cell that you should execute to run this notebook on the cluster.\n",
    "\n",
    "DO NOT EXECUTE THIS MULTIPLE TIMES. IT WILL SUBMIT MULTIPLE JOBS TO THE CLUSTER.\n",
    "PLEASE BE RESPONSIBLE.\n",
    "\"\"\"\n",
    "# We need this to grab the current working directory.\n",
    "import os\n",
    "\n",
    "# We'll need this to run the program that will submit the cluster job.\n",
    "# This module isn't needed for the calculation itself, which is why it is\n",
    "# imported here.\n",
    "import subprocess\n",
    "\n",
    "# First, we save this as \"map.py\". Make sure it doesn't already exist.\n",
    "try:\n",
    "    os.remove(\"map.py\")\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "# Convert this notebook to a python script in our home directory.\n",
    "!jupyter nbconvert --to script map.ipynb\n",
    "\n",
    "\n",
    "# Submit the job, which in turn loads the Hamilton module and runs:\n",
    "# qsub -pe smp 40 -l m_mem_free=2.5G -P i07 cluster_job.sh\n",
    "subprocess.run(\n",
    "    [\"sh\", \"/dls_sw/apps/fast_rsm/current/scripts/cluster_sub.sh\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The following cells contain convenience tools for e.g. interacting with and\n",
    "visualising data.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "**RUN THIS**\n",
    "\n",
    "You should run this cell, but don't worry about its contents.\n",
    "\n",
    "This cell just defines a few handy functions that will be used below.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def most_recent_cluster_output():\n",
    "    \"\"\"\n",
    "    Returns the filename of the most recent cluster stdout output.\n",
    "    \"\"\"\n",
    "    # Get all the cluster job files that have been created.\n",
    "    files = [x for x in os.listdir() if x.startswith(\"cluster_job.sh.o\")]\n",
    "    numbers = [int(x[16:]) for x in files]\n",
    "\n",
    "    # Work out which cluster job is the most recent.\n",
    "    most_recent_job_no = np.max(numbers)\n",
    "    most_recent_file = \"\"\n",
    "    for file in files:\n",
    "        if str(most_recent_job_no) in file:\n",
    "            most_recent_file = file\n",
    "\n",
    "    return most_recent_file\n",
    "\n",
    "\n",
    "def most_recent_cluster_error():\n",
    "    \"\"\"\n",
    "    Returns the filename of the most recent cluster stderr output.\n",
    "    \"\"\"\n",
    "    # Get all the cluster job files that have been created.\n",
    "    files = [x for x in os.listdir() if x.startswith(\"cluster_job.sh.e\")]\n",
    "    numbers = [int(x[16:]) for x in files]\n",
    "\n",
    "    # Work out which cluster job is the most recent.\n",
    "    most_recent_job_no = np.max(numbers)\n",
    "    most_recent_file = \"\"\n",
    "    for file in files:\n",
    "        if str(most_recent_job_no) in file:\n",
    "            most_recent_file = file\n",
    "\n",
    "    return most_recent_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Are we nearly there yet?\n",
    "\n",
    "Executing this cell tells you if your most recent cluster submission has\n",
    "finished executing.\n",
    "\"\"\"\n",
    "\n",
    "most_recent_file = most_recent_cluster_output()\n",
    "\n",
    "# Open that file, and see if it ends in \"Finished.\"\n",
    "with open(most_recent_file, 'r') as f:\n",
    "    lines = f.read().splitlines()\n",
    "    \n",
    "    # Check if there's nothing in the file yet.\n",
    "    if len(lines) == 0:\n",
    "        print(\"Processing either not started or crashed.\")\n",
    "    last_line = lines[-1].strip()\n",
    "    if last_line.startswith(\"PROCESSING FINISHED.\"):\n",
    "        print(\"Most recent processing has completed.\")\n",
    "    else:\n",
    "        print(\n",
    "            \"Processing job has started on the cluster, but isn't finished. \"\n",
    "            \"It could have crashed, or it could still be running.\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell contains functions that you can use to read the most recent cluster\n",
    "output and error logs.\n",
    "\"\"\"\n",
    "\n",
    "def print_recent_output():\n",
    "    \"\"\"Prints the most recent cluster output log file.\"\"\"\n",
    "    with open(most_recent_cluster_output(), 'r') as f:\n",
    "        lines = f.read().splitlines()\n",
    "        for line in lines:\n",
    "            print(line)\n",
    "\n",
    "def print_recent_error():\n",
    "    \"\"\"Prints the most recent cluster error log file.\"\"\"\n",
    "    with open(most_recent_cluster_error(), 'r') as f:\n",
    "        lines = f.read().splitlines()\n",
    "        for line in lines:\n",
    "            print(line)\n",
    "\n",
    "print(\"Cluster stderr (error logs) below:\")\n",
    "print_recent_error()\n",
    "print(\"\\n\\n\\nCluster stdout (output logs) below:\")\n",
    "print_recent_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell computes projections on your data (e.g. maps it to I vs Q, I vs tth,\n",
    "I vs L, qparalel_qperpendicular*). Intensity is logged, but just comment out\n",
    "the line of code that logs it to plot linear (ctrl+f np.log).\n",
    "\n",
    "If you want structure factor, replace np.log with np.sqrt.\n",
    "\n",
    "*coming soon TM\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "from diffraction_utils import I07Nexus\n",
    "from fast_rsm.binning import weighted_bin_1d\n",
    "from fast_rsm.diamond_utils import get_volume_and_bounds, q_to_theta\n",
    "\n",
    "\n",
    "# This is the path to the .npy file containing your data.\n",
    "path_to_npy = \"/path/to/file.npy\"\n",
    "\n",
    "# Please only set one of these to be True, or something will probably go wrong.\n",
    "map_to_q = True\n",
    "map_to_tth = False\n",
    "map_to_l = False\n",
    "\n",
    "# If you want to map to tth, please specify the energy of the beam (in eV).\n",
    "# If not, don't worry about this value because it wont be used.\n",
    "beam_energy = None\n",
    "\n",
    "# Alternatively, to grab the beam energy from a nexus file, use this code.\n",
    "# path_to_nx = \"/path/to/nx\"\n",
    "# beam_energy = I07Nexus(path_to_nx).probe_energy\n",
    "\n",
    "# If you want to set bin_size, then set number_of_bins = None.\n",
    "# If you set both, bin_size will win.\n",
    "number_of_bins = 1000\n",
    "bin_size = None\n",
    "\n",
    "# BELOW ARE JUST DETAILS OF THE CALCULATION.\n",
    "\n",
    "# Grab the volume, and the bounds on the volume.\n",
    "volume, start, stop, step = get_volume_and_bounds(path_to_npy)\n",
    "\n",
    "# Calculate Q everywhere.\n",
    "q_x = np.arange(start[0], stop[0], step[0], dtype=np.float32)\n",
    "q_y = np.arange(start[1], stop[1], step[1], dtype=np.float32)\n",
    "q_z = np.arange(start[2], stop[2], step[2], dtype=np.float32)\n",
    "if map_to_l:\n",
    "    q_x *= 0\n",
    "    q_y *= 0\n",
    "q_x, q_y, q_z = np.meshgrid(q_x, q_y, q_z, indexing='ij')\n",
    "\n",
    "# Now we can work out the |Q| for each voxel.\n",
    "q_x_squared = np.square(q_x)\n",
    "q_y_squared = np.square(q_y)\n",
    "q_z_squared = np.square(q_z)\n",
    "q_lengths = np.sqrt(q_x_squared + q_y_squared + q_z_squared)\n",
    "\n",
    "# It doesn't make sense to keep the 3D shape because we're about to map\n",
    "# to a 1D space anyway.\n",
    "volume = volume.ravel()\n",
    "q_lengths = q_lengths.ravel()\n",
    "\n",
    "# If we're mapping to tth, map q->tth now.\n",
    "if map_to_tth:\n",
    "    q_lengths = 2*q_to_theta(q_lengths, beam_energy)\n",
    "\n",
    "# If we haven't been given a bin_size, we need to calculate it.\n",
    "min_q = float(np.min(q_lengths))\n",
    "max_q = float(np.max(q_lengths))\n",
    "\n",
    "if bin_size is None:\n",
    "    # Work out the bin_size from the range of |Q| values.\n",
    "    bin_size = (max_q - min_q)/number_of_bins\n",
    "\n",
    "# Now that we know the bin_size, we can make an array of the bin edges.\n",
    "bins = np.arange(min_q, max_q, bin_size)\n",
    "\n",
    "# Use this to make intensity & count arrays of the correct size.\n",
    "intensity = np.zeros_like(bins, dtype=np.float32)\n",
    "count = np.zeros_like(intensity, dtype=np.uint32)\n",
    "\n",
    "# Do the binning. This can take a long time to execute, potentially, since we're\n",
    "# binning 100 million elements or so (depending on your volume size).\n",
    "intensity = weighted_bin_1d(\n",
    "    q_lengths, volume, intensity, count, min_q, max_q, bin_size)\n",
    "\n",
    "# Normalise by the counts.\n",
    "intensity /= count.astype(np.float32)\n",
    "\n",
    "# Take the log.\n",
    "intensity = np.log(intensity)\n",
    "\n",
    "if map_to_q:\n",
    "    labels = {'x': \"Q(1/Ã…)\", 'y': 'ln(Intensity) (a.u.)'}\n",
    "if map_to_l:\n",
    "    labels = {'x': \"l (unitless)\", 'y': 'ln(Intensity) (a.u.)'}\n",
    "if map_to_tth:\n",
    "    labels = {'x': \"2Î¸ (degrees)\", 'y': 'ln(Intensity) (a.u.)'}\n",
    "# Finally, do the plot!\n",
    "fig = px.line(x=bins, y=intensity, labels=labels)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell is for working out background regions for background subtraction\n",
    "along l. You can do this in binoculars, though.\n",
    "\"\"\"\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "from fast_rsm.diamond_utils import get_volume_and_bounds, q_to_theta\n",
    "\n",
    "# This is the path to the .npy file containing your data.\n",
    "path_to_npy = \"/path/to/file.npy\"\n",
    "l_value = 1\n",
    "# Options are 'log', 'linear' and 'sqrt'\n",
    "plot_mode = 'log'\n",
    "\n",
    "# Grab the volume, and the bounds on the volume.\n",
    "volume, start, stop, step = get_volume_and_bounds(path_to_npy)\n",
    "\n",
    "# Replace NaN with 0.\n",
    "volume = np.nan_to_num(volume)\n",
    "\n",
    "def l_to_arr_coord(l_value: float, start, step):\n",
    "    \"\"\"\n",
    "    Convenience function for converting between an l value and an array\n",
    "    coordinate. Useful for for h, k slices at a specific value of l.\n",
    "    \"\"\"\n",
    "    return int((l_value - start[2])/step[2])\n",
    "\n",
    "l_coordinate = l_to_arr_coord(l_value, start, step)\n",
    "\n",
    "image = volume[:, :, l_coordinate]\n",
    "\n",
    "if plot_mode not in ['log', 'linear', 'sqrt']:\n",
    "    raise ValueError(\n",
    "        f\"plot_mode must be one of 'log', 'linear' or sqrt'. Got {plot_mode}.\")\n",
    "\n",
    "if plot_mode == 'log':\n",
    "    image = np.log(image)\n",
    "if plot_mode == 'sqrt':\n",
    "    image = np.sqrt(image)\n",
    "\n",
    "fig = px.imshow(image)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell is for carrying out background subtraction along l. You should run\n",
    "the previous cell first to take a look at (and load) your volume.\n",
    "\"\"\"\n",
    "\n",
    "from diffraction_utils import Region\n",
    "\n",
    "# These regions shouldn't overlap, or you'll subtract the pixels in the overlap\n",
    "# regions multiple times.\n",
    "signal_region = Region(1, 20, 15, 40)\n",
    "bkg_region_1 = Region(1, 20, 15, 40)\n",
    "bkg_region_2 = Region(1, 20, 15, 40)\n",
    "bkg_regions = [bkg_region_1, bkg_region_2]\n",
    "\n",
    "for l in range(volume.shape[-1]):\n",
    "    # Make a slice along this value of l.\n",
    "    l_slice = volume[:, :, l]\n",
    "\n",
    "    # Calculate the weighted average of the noise in the background regions.\n",
    "    means = [np.mean(l_slice[x.slice]) for x in bkg_regions]\n",
    "    num_pixels = [x.num_pixels for x in bkg_regions]\n",
    "    weighted_mean = np.sum(\n",
    "        [means[i] * x for i, x in enumerate(num_pixels)])/np.sum(num_pixels)\n",
    "\n",
    "    # Subtract this from the l slice.\n",
    "    l_slice -= weighted_mean\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('fast_rsm')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "6d3d0365b0cf09d8aafe0187e5e70cb1582c70373590192715db7909432628a2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
